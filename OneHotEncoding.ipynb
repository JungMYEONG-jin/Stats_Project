{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OneHotEncoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLGZLaX6gNJLvxP5hMzQly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JungMYEONG-jin/Stats_Project/blob/window/OneHotEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv79hINzF7j3"
      },
      "source": [
        "import tensorflow"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtVLBuF_GFPl"
      },
      "source": [
        "# One-hot encoding of words or characters\n",
        "\n",
        "This notebook contains the first code sample found in Chapter 6, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "One-hot encoding is the most common, most basic way to turn a token into a vector. You already saw it in action in our initial IMDB and \n",
        "Reuters examples from chapter 3 (done with words, in our case). It consists in associating a unique integer index to every word, then \n",
        "turning this integer index i into a binary vector of size N, the size of the vocabulary, that would be all-zeros except for the i-th \n",
        "entry, which would be 1.\n",
        "\n",
        "Of course, one-hot encoding can be done at the character level as well. To unambiguously drive home what one-hot encoding is and how to \n",
        "implement it, here are two toy examples of one-hot encoding: one for words, the other for characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip70fLUhGBNl"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework\"]\n",
        "\n",
        "token_index = {}\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Me-BfltGPT7",
        "outputId": "07ab11cf-c4e1-4d07-bf47-0c3c7fa205b4"
      },
      "source": [
        "for i in samples:\n",
        "  for word in i.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index)+1\n",
        "      # 중복제거한 단어만 저장\n",
        "\n",
        "token_index"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The': 1,\n",
              " 'ate': 8,\n",
              " 'cat': 2,\n",
              " 'dog': 7,\n",
              " 'homework': 10,\n",
              " 'mat.': 6,\n",
              " 'my': 9,\n",
              " 'on': 4,\n",
              " 'sat': 3,\n",
              " 'the': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgwYRDgJGi_p"
      },
      "source": [
        "# vectorize our samples\n",
        "# only consider max_len\n",
        "\n",
        "max_len = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_len, max(token_index.values())+1))\n",
        "\n",
        "for i, j in enumerate(samples):\n",
        "  for k, word in list(enumerate(j.split()))[:max_len]:\n",
        "    index = token_index.get(word)\n",
        "    results[i,k,index] = 1."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1deqXhepHNIo",
        "outputId": "724e64aa-49c6-4a6e-e7f5-06ffd5aab222"
      },
      "source": [
        "results\n",
        "# The cat on the mat\n",
        "# The dog ate my homework"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "dpYJvHxJHOQL",
        "outputId": "af03f258-e953-4e2b-ecea-096939b877b0"
      },
      "source": [
        "import string\n",
        "\n",
        "characters = string.printable\n",
        "\n",
        "characters"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI0y3gkVHdMp"
      },
      "source": [
        "token_index = dict(zip(characters, range(1, len(characters)+1)))\n",
        "\n",
        "max_len = 50\n",
        "results = np.zeros((len(samples), max_len, max(token_index.values())+1))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP96BtGuHvE9"
      },
      "source": [
        "for i, j in enumerate(samples):\n",
        "  for k, character in enumerate(j[:max_len]):\n",
        "    index = token_index.get(character)\n",
        "    results[i,k,index] = 1."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bwu3h1EIBNw",
        "outputId": "63c07de0-6250-41e7-ea51-1fd78bae0f5e"
      },
      "source": [
        "results"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKwjYSj9IKHh"
      },
      "source": [
        "Note that Keras has built-in utilities for doing one-hot encoding text at the word level or character level, starting from raw text data. \n",
        "This is what you should actually be using, as it will take care of a number of important features, such as stripping special characters \n",
        "from strings, or only taking into the top N most common words in your dataset (a common restriction to avoid dealing with very large input \n",
        "vector spaces)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLHhXIACIFyb"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MumgN7DjIPqV",
        "outputId": "100ab686-50a9-49ce-bfd6-645e461ed8aa"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "seq = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "oh_res = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"%s unique tokens\" %len(word_index))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 unique tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwOzO9anIum5"
      },
      "source": [
        "\n",
        "A variant of one-hot encoding is the so-called \"one-hot hashing trick\", which can be used when the number of unique tokens in your \n",
        "vocabulary is too large to handle explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these \n",
        "indices in a dictionary, one may hash words into vectors of fixed size. This is typically done with a very lightweight hashing function. \n",
        "The main advantage of this method is that it does away with maintaining an explicit word index, which \n",
        "saves memory and allows online encoding of the data (starting to generate token vectors right away, before having seen all of the available \n",
        "data). The one drawback of this method is that it is susceptible to \"hash collisions\": two different words may end up with the same hash, \n",
        "and subsequently any machine learning model looking at these hashes won't be able to tell the difference between these words. The likelihood \n",
        "of hash collisions decreases when the dimensionality of the hashing space is much larger than the total number of unique tokens being hashed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe0Pvq0RIqvn"
      },
      "source": [
        "dim = 1000\n",
        "max_len = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_len, dim))\n",
        "for i, j in enumerate(samples):\n",
        "  for k, word in list(enumerate(j.split()))[:max_len]:\n",
        "    index = abs(hash(word)) % dim\n",
        "    results[i, k, index] = 1.\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6c7N3ndJDl3",
        "outputId": "8073a1fd-c7ed-4493-86f6-4a2bce036a45"
      },
      "source": [
        "results"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpS-ATcCJENN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}