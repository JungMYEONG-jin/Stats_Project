{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "전처리.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JungMYEONG-jin/Stats_Project/blob/window/%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-h3JIv9bOyf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, Dropout, Input, Conv1D, MaxPooling1D, Bidirectional, GlobalMaxPool1D, SpatialDropout1D\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, Dropout, Input, Conv1D, MaxPooling1D, Bidirectional, GlobalMaxPool1D, SpatialDropout1D\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/nhn/news_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynAS_pUqe2mQ"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNjuPd1FbOyo"
      },
      "source": [
        "train.drop({'n_id', 'date'}, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrjlDed5bOyo"
      },
      "source": [
        "train['content'] = train['title'] + train['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLCunvWKbOyp"
      },
      "source": [
        "# 띄어쓰기 제거 및 [] () ? %&@이상 기호 제거\n",
        "train['content'] = train['content'].str.replace(\" \", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZBJL1tobOyp"
      },
      "source": [
        "train['content'] = train['content'].str.replace(\"[\",\"\").str.replace(\"]\",\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4g6T6hubOyq"
      },
      "source": [
        "import re\n",
        "train['content'] = train['content'].apply(lambda x : re.sub(r\"[()]\", \"\", x))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQrj0q5ygCyK"
      },
      "source": [
        "pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xormypyigXVv"
      },
      "source": [
        "from pykospacing import spacing\n",
        "\n",
        "print(spacing(train['content'][313]))\n",
        "\n",
        "def make_space(texts):\n",
        "  lst=[]\n",
        "  for text in texts:\n",
        "    res = spacing(text)\n",
        "    lst.append(res)\n",
        "    print(res)\n",
        "  return lst\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVD1Kg-vg98q"
      },
      "source": [
        "space_lst = make_space(train['content'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5-kq9sPbOyq"
      },
      "source": [
        "train['content'] = train['content'].apply(lambda x : re.sub(r\"[@$&]\", \"\", x)) # 기호 제거해야 한스펠 사용  가능"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxEKSSu0g84y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43qTr3fWbOyq"
      },
      "source": [
        "#train['content'] = train['content'].apply(lambda x : re.sub(r\"<[^>]+>\", \"\", x))\n",
        "#train['content'] = train['content'].apply(lambda x : re.sub(r\"[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\:\\;\\!\\_\\~\\$\\'\\\"]\", \"\", x))\n",
        "train['content'] = train['content'].apply(lambda x : re.sub(r\"\\t\", \"\", x))\n",
        "train['content'] = train['content'].apply(lambda x : re.sub(r\"\\n\", \"\", x))\n",
        "train['content'] = train['content'].apply(lambda x : re.sub(r\"\\d+\", \"\", x))\n",
        "# 스펠 체커 자꾸 특수문자 오류떠서 마침표 콤마 제외하고 다 제외시켜버리자\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnBKBRjNbOyr"
      },
      "source": [
        "def text_token_process(texts):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는' , '데', '등','null']\n",
        "    #mecab = Mecab() # 일단 미캡사용\n",
        "    mecab = Mecab()\n",
        "    token_lst = []\n",
        "    \n",
        "    for text in texts:\n",
        "        token = mecab.morphs(text) # 형태소 변환\n",
        "        token = [t for t in token if t not in stopwords or type(t) !=float] # 불용어 걸러주기\n",
        "        token_lst.append(token)\n",
        "    return token_lst, mecab\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICoQWMkPbOyr"
      },
      "source": [
        "def normalize(data):\n",
        "    normal=[]\n",
        "    for i in data:\n",
        "        i = i.lower()\n",
        "        normal.append(i)\n",
        "    return normal\n",
        "\n",
        "reg_con = normalize(train['content'])\n",
        "\n",
        "#train.drop('title', axis=1, inplace = True)\n",
        "train['content'] = reg_con\n",
        "\n",
        "train['new_content'], mecab = text_token_process(train['content'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TyrqjOPbOys"
      },
      "source": [
        "train.drop('title', axis=1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10e7DMVYbOys"
      },
      "source": [
        "y = train['info'] # target설정\n",
        "\n",
        "\n",
        "\n",
        "y\n",
        "\n",
        "\n",
        "maxlen = sum(len(i) for i in train['new_content'])\n",
        "print(maxlen/len(train['new_content']))\n",
        "print(len(train['new_content'][45644]))\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmIuUo2HbOys"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=1000):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len, padding='post') #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['new_content'], max_len = 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQaPPXMjbOyt"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G1-mmX1bOyt"
      },
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size,32),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(16)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu',kernel_regularizer = regularizers.l2(0.001)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "# 이건 나중에 Bidirectional(LSTM(16)),\n",
        "opt = Adam(learning_rate=0.01)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy4QdXC0bOyu"
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# lr 001 아담\n",
        "history = model.fit(x_train, y_train, epochs=10,validation_split=0.1, batch_size=90, callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDY9mjwybOyu"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('model_token_200.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4nklnwybOyu"
      },
      "source": [
        "\n",
        "test = pd.read_csv('/home/dktlsk6/Downloads/데이터분석/open/news_test.csv')\n",
        "\n",
        "test['content'] = test['title']+test['content']\n",
        "\n",
        "test.drop({'n_id', 'date', 'id', 'title'}, axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GQW65jKbOyv"
      },
      "source": [
        "reg_con = normalize(test['content'])\n",
        "\n",
        "#train.drop('title', axis=1, inplace = True)\n",
        "test['content'] = reg_con\n",
        "test['content'] = test['content'].str.replace(\"[\",\"\").str.replace(\"]\",\"\")\n",
        "test['content'] = test['content'].apply(lambda x : re.sub(r\"[()]\", \"\", x))\n",
        "test['content'] = test['content'].apply(lambda x : re.sub(r\"[@$&]\", \"\", x))\n",
        "#test['content'] = test['content'].apply(lambda x : re.sub(r\"<[^>]+>\", \"\", x))\n",
        "#test['content'] = test['content'].apply(lambda x : re.sub(r\"[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\:\\;\\!\\_\\~\\$\\'\\\"]\", \"\", x))\n",
        "test['content'] = test['content'].apply(lambda x : re.sub(r\"\\t\", \"\", x))\n",
        "test['content'] = test['content'].apply(lambda x : re.sub(r\"\\n\", \"\", x))\n",
        "test['content'] = test['content'].apply(lambda x : re.sub(r\"\\d+\", \"\", x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sIZcIaVbOyv"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1uGnkKebOyv"
      },
      "source": [
        "# 토큰화\n",
        "\n",
        "test['new_content'], mecab = text_token_process(test['content'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRvUnvwubOyw"
      },
      "source": [
        "test_X_seq = vectorizer.texts_to_sequences(test['new_content'])\n",
        "\n",
        "X_test = pad_sequences(test_X_seq, maxlen=200, padding='post')\n",
        "\n",
        "\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNQuWJgibOyw"
      },
      "source": [
        "pred = model.predict_classes(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVnwx98AbOyw"
      },
      "source": [
        "print('진짜뉴스 ', len(pred[pred==0]))\n",
        "print('가짜뉴스 ', len(pred[pred==1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O_lPa5xbOyw"
      },
      "source": [
        "sample = pd.read_csv('/home/dktlsk6/Downloads/데이터분석/open/sample_submission.csv')\n",
        "sample['info'] = pred\n",
        "sample.to_csv('test_13.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3oWYkT7bOyx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}